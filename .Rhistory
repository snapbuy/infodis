install.packages(`torch`)
install.packages(`pytorch`)
demo(images)
demo()
demo(image())
install.packages("torch")
install.packages("tidyverse")
library(torch)
x <- torch_empty(5, 3)
torch_tensor()
x
dim(x)
rand_tensor <- torch_rand(5, 3)
rnad_tensor
rand_tensor
cuda_is_available()
library(torch)
install_torch("ggplot2")
install_torch("ggplot")
install.packages("ggplot")
install.packages("ggplot2")
library(ggplot2)
install.packages(c("boot", "broom", "class", "cli", "cluster", "codetools", "foreign", "KernSmooth", "lubridate", "MASS", "Matrix", "mgcv", "nlme", "nnet", "pillar", "pkgload", "rmarkdown", "spatial", "tibble"))
install.packages(c("boot", "broom", "class", "cli", "cluster", "codetools", "foreign", "KernSmooth", "lubridate", "MASS", "Matrix", "mgcv", "nlme", "nnet", "pillar", "pkgload", "rmarkdown", "spatial", "tibble"))
install.packages(c("farver", "ps", "tidyr"))
install.packages(c("farver", "ps", "tidyr"))
q()
install.packages(c("desc", "dplyr", "isoband", "pillar", "rvest", "tinytex", "waldo"))
install.packages("tsibbledata")
library(tidyverse)
library(lubridate)
library(feasts) # Feature Extraction and Statistics for Time Series
library(tsibbledata) # Diverse Datasets for 'tsibble'
vic_elec %>% glimpse()
vic_elec_2014 <-  vic_elec %>%
filter(year(Date) == 2014) %>%
select(-c(Date, Holiday)) %>%
mutate(Demand = scale(Demand), Temperature = scale(Temperature)) %>%
pivot_longer(-Time, names_to = "variable") %>%
update_tsibble(key = variable)
vic_elec_2014 %>% filter(month(Time) == 7) %>%
autoplot() +
scale_colour_manual(values = c("#08c5d1", "#00353f")) +
theme_minimal()
library(tidyverse)
install.packages(c("utf8", "xfun"))
library(palmerpenguins)
+
install.packages("devtools")
library(palmerpenguins)
install.packages("devtools")
library(palmerpenguins)
devtools::install_github("allisonhorst/")
devtools::install_github("allisonhorst/palemrpanguins")
clear()
devtools::install_github("allisonhorst/palemrpanguins")
devtools::install_github("xiaodaigh/disk.frame")
devtools::install_github("xiaodaigh/disk.frame")
# tidy한 방법으로 데이터 다루기
## 패키지 설치
install.packages("tidyverse")
### github에 있는 패키지를 설치하기 위한 패키지 설치
install.packages("remotes")
### github에 있는 패키지를 설치하기 위한 패키지 설치
install.packages("devtools")
install.packages(c("benchmarkme", "broom", "callr", "cli", "cpp11", "dbplyr", "diffobj", "disk.frame", "fabletools", "feasts", "gert", "gh", "pkgload", "processx", "reprex", "slider", "survival", "tinytex", "tsibbledata", "vctrs"))
library(keras)
library(mlbench)
library(dplyr)
library(magrittr)
library(neuralnet)
install.packages("keras")
install.packages("mlbench")
install.packages(neuralnet)
install.packages("neuralnet")
library(keras)
library(mlbench)
library(dplyr)
library(magrittr)
library(neuralnet)
data("BostonHousing")
data <- BostonHousing
str(data)
n <- neuralnet(medv ~ crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+b+lstat,
data = data,
hidden = c(12,7),
linear.output = F,
lifesign = 'full',
rep=1)
plot(n,col.hidden = 'darkgreen',
col.hidden.synapse = 'darkgreen',
show.weights = F,
information = F,
fill = 'lightblue')
install.packages("quantmod")
library(quantmod)
getSymbols('AAPL')
head(AAPL)
View(AAPL)
getSymbols('SPY')
head(SPY)
tail(SPY)
chart_Series(Ad(SPY))
chart_Series(Ad(AAPL))
chart_Series(Ad(SPY))
library(quantmod)
getSymbols('DGS10', src = 'FRED')
DGS10 %>% chart_Series()
library(magrittr)
DGS10 %>% chart_Series()
View(DGS10)
View(DGS10)
install.packages("digest")
library(digest)
library("dplyr")
install.packages(c("pillar", "tsibble", "viridisLite"))
View(SPY)
View(SPY)
View(SPY)
View(AAPL)
shiny::runGadget(sparklyr::connection_spark_shinyapp(), viewer = .rs.embeddedViewer)
shiny::runGadget(sparklyr::connection_spark_shinyapp(), viewer = .rs.embeddedViewer)
library(sparklyr)
library(dplyr)
spark_install(version = "2.4.3", hadoop_version = "2.7")
sc <- spark_connect(master = "local")
library(digest)
setwd("/Volumes/T7/infodis")
dsn1 = read.csv("금융정보.csv", header = T, fileEncoding = "utf-8")
dsn2 = read.csv("병원정보.csv", header = T, fileEncoding = "utf-8")
library(digest)
dsn1$combine_key = sapply(paste(dsn1$Name, dsn1$BirthDate), digest, algo="sha256")
dsn2$combine_key = sapply(paste(dsn2$Name, dsn2$BirthDate), digest, algo="sha256")
dsn1$combine_key[1]
dsn2$combine_key[1]
library("dplyr")
dsn1_key = select(dsn1, c(ID, combine_key))
dsn2_key = select(dsn2, c(ID, combine_key))
write.csv(dsn1_key, file="금융정보_key.csv", row.names=FALSE, fileEncoding='utf-8')
write.csv(dsn2_key, file="병원정보_key.csv", row.names=FALSE, fileEncoding='utf-8')
setwd("/Volumes/T7/infodis")
dsn = read.csv("금융정보.csv", header = T, fileEncoding = "utf-8")
library("dplyr")
# 컬럼 삭제
#dsn = select(dsn, -c(ID, Name, BirthDate))
dsn = select(dsn, -c(Name, BirthDate))
# 나이 범주화
dsn$AgeGrp <- NA
dsn$AgeGrp <- ifelse(dsn$Age < 20, 0, dsn$AgeGrp)
dsn$AgeGrp <- ifelse(dsn$Age >= 20 & dsn$Age <25, 1, dsn$AgeGrp)
dsn$AgeGrp <- ifelse(dsn$Age >= 25 & dsn$Age <30, 2, dsn$AgeGrp)
dsn$AgeGrp <- ifelse(dsn$Age >= 30 & dsn$Age <35, 3, dsn$AgeGrp)
dsn$AgeGrp <- ifelse(dsn$Age >= 35 & dsn$Age <40, 4, dsn$AgeGrp)
dsn$AgeGrp <- ifelse(dsn$Age >= 40 & dsn$Age <45, 5, dsn$AgeGrp)
dsn$AgeGrp <- ifelse(dsn$Age >= 45 & dsn$Age <50, 6, dsn$AgeGrp)
dsn$AgeGrp <- ifelse(dsn$Age >= 50 & dsn$Age <55, 7, dsn$AgeGrp)
dsn$AgeGrp <- ifelse(dsn$Age >= 55 & dsn$Age <60, 8, dsn$AgeGrp)
dsn$AgeGrp <- ifelse(dsn$Age >= 60, 9, dsn$AgeGrp)
dsn$Age <- dsn$AgeGrp
dsn = select(dsn, -c(AgeGrp))
# 특이정보 처리(범주형변수)
table(dsn$Job)
dsn$Job = ifelse(dsn$Job == "국회의원", "급여_공공", dsn$Job)
table(dsn$Job)
# 특이정보 처리(연속형변수) --> 범주화로 처리 혹은 특이값이 아닌 최대값으로 변경 대체
boxplot(dsn$Nodebt)
boxplot.stats(dsn$Nodebt)$out
result <- boxplot.stats(dsn$Nodebt)
result
dsn$Nodebt <- ifelse(dsn$Nodebt > 30, 30, dsn$Nodebt)
boxplot(dsn$Nodebt)
# Rounding(천원단위)
boxplot(dsn$Income)
dsn$Income <- round(dsn$Income/10**3, digits = 0) * 10**3
boxplot(dsn$Overdue)
dsn$Overdue <- round(dsn$Overdue/10**2, digits = 0) * 10**2
boxplot(dsn$Income)
boxplot(dsn$Overdue)
# Rounding(정수화)
dsn$CreditScore <- round(dsn$CreditScore)
#write.csv(dsn, file="금융정보_Pseudonymization.csv", row.names=FALSE, fileEncoding='utf-8')
write.csv(dsn, file="금융정보_결합요청.csv", row.names=FALSE, fileEncoding='utf-8')
df <- read.csv("adult.csv", header=TRUE)
summary(df)
# Quasi-Identifier: age, workclass, marital_status, race, sex, native_country
# sensitive variable: salary_class
library(sqldf)
freq <- sqldf("select count(*) from df group by age, workclass, marital_status, race, sex, native_country")
table(freq$`count(*)`)
sqldf("select age, count(*) from df group by age")
sqldf("select workclass, count(*) from df group by workclass")
sqldf("select marital_status, count(*) from df group by marital_status")
sqldf("select race, count(*) from df group by race")
sqldf("select sex, count(*) from df group by sex")
sqldf("select native_country, count(*) from df group by native_country")
library(gmodels)
CrossTable(x = df$workclass, y = df$salary_class, prop.t=FALSE, expected=TRUE, chisq =TRUE)
CrossTable(x = df$marital_status, y = df$salary_class, prop.t=FALSE, expected=TRUE, chisq =TRUE)
CrossTable(x = df$race, y = df$salary_class, prop.t=FALSE, expected=TRUE, chisq =TRUE)
CrossTable(x = df$sex, y = df$salary_class, prop.t=FALSE, expected=TRUE, chisq =TRUE)
CrossTable(x = df$native_country, y = df$salary_class, prop.t=FALSE, expected=TRUE, chisq =TRUE)
setwd("/Volumes/T7/infodis")
df <- read.csv("adult.csv", header=TRUE)
summary(df)
# Quasi-Identifier: age, workclass, marital_status, race, sex, native_country
# sensitive variable: salary_class
library(sqldf)
freq <- sqldf("select count(*) from df group by age, workclass, marital_status, race, sex, native_country")
table(freq$`count(*)`)
sqldf("select age, count(*) from df group by age")
sqldf("select workclass, count(*) from df group by workclass")
sqldf("select marital_status, count(*) from df group by marital_status")
sqldf("select race, count(*) from df group by race")
sqldf("select sex, count(*) from df group by sex")
sqldf("select native_country, count(*) from df group by native_country")
library(gmodels)
CrossTable(x = df$workclass, y = df$salary_class, prop.t=FALSE, expected=TRUE, chisq =TRUE)
CrossTable(x = df$marital_status, y = df$salary_class, prop.t=FALSE, expected=TRUE, chisq =TRUE)
CrossTable(x = df$race, y = df$salary_class, prop.t=FALSE, expected=TRUE, chisq =TRUE)
CrossTable(x = df$sex, y = df$salary_class, prop.t=FALSE, expected=TRUE, chisq =TRUE)
CrossTable(x = df$native_country, y = df$salary_class, prop.t=FALSE, expected=TRUE, chisq =TRUE)
setwd("/Volumes/T7/infodis")
df <- read.csv("arxTest.csv", header=TRUE)
library(sqldf)
# (0,0,0) 모형의 frequency
sqldf("select count(*) from df group by sex, age, loc")
df$sexL1 <- '*'
df$ageL1 <- NA
df$ageL1 <- ifelse(df$age < 20, 0, NA)
df$ageL1 <- ifelse(df$age >= 20 & df$age <25, 1, df$ageL1)
df$ageL1 <- ifelse(df$age >= 25 & df$age <30, 2, df$ageL1)
df$ageL1 <- ifelse(df$age >= 30 & df$age <35, 3, df$ageL1)
df$ageL1 <- ifelse(df$age >= 35 & df$age <40, 4, df$ageL1)
df$ageL1 <- ifelse(df$age >= 40 & df$age <45, 5, df$ageL1)
df$ageL1 <- ifelse(df$age >= 45 & df$age <50, 6, df$ageL1)
df$ageL1 <- ifelse(df$age >= 50 & df$age <55, 7, df$ageL1)
df$ageL1 <- ifelse(df$age >= 55 & df$age <60, 8, df$ageL1)
df$ageL1 <- ifelse(df$age >= 60, 9, df$ageL1)
df$locL1 <- df$loc
df$locL1 <- ifelse(df$loc != "서울" & df$loc != "경기", "수도권외",df$locL1)
df$locL1 <- ifelse(df$loc == "서울", "서울",df$locL1)
df$locL1 <- ifelse(df$loc == "경기", "경기",df$locL1)
# (0,1,1) 모형의 frequency
sqldf("select sex, ageL1, locL1, count(*) from df group by sex, ageL1, locL1")
# (1,0,1) 모형의 frequency
sqldf("select sexL1, age, locL1, count(*) from df group by sexL1, age, locL1")
View(df)
View(df)
df <- read.csv("arxTest.csv", header=TRUE, fileEncoding = "utf-8")
setwd("/Volumes/T7/infodis")
# KISA는 두 기관에서 받은 파일로 키와 ID 결합정보를 생성한다.
key1 = read.csv("금융정보_key.csv", header = T, fileEncoding = "utf-8")
key2 = read.csv("병원정보_key.csv", header = T, fileEncoding = "utf-8")
key = merge(key1, key2, by = 'combine_key')
# 결합처리는 두 기관에서 받은 정보를 KISA 결합키를 기준으로 병합한다.
dsn1 = read.csv("금융정보_결합요청.csv", header = T, fileEncoding = "utf-8")
dsn2 = read.csv("병원정보_결합요청.csv", header = T, fileEncoding = "utf-8")
library("dplyr")
dsn1 = rename(dsn1, ID.x = ID)
dsn2 = rename(dsn2, ID.y = ID)
dsn_combine = merge(key, dsn1, by = 'ID.x')
dsn_combine = merge(dsn_combine, dsn2, by = 'ID.y')
dsn_combine = select(dsn_combine, -c(ID.x, ID.y, combine_key))
write.csv(dsn_combine, file="결합결과정보.csv", row.names=FALSE, fileEncoding='utf-8')
View(result)
View(key1)
View(key)
View(dsn_combine)
